{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8d671fd8d160>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "# === Modules import === # \n",
    "!pip install cumulator\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from cumulator import base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN we are using for our task\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.fc1 = nn.Linear(2048, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "# This class represents the gradient for on NN model. Keeping Gradient for each layer distinctly stored. \n",
    "# It also allows us to compute easily the relative distance between the gradient of two agents for Weight Erosion.\n",
    "class GradientStocker:\n",
    "    def __init__(self, model_names):\n",
    "        for item in model_names:\n",
    "            setattr(self, item, 0)\n",
    "\n",
    "    def get_attributes(self):\n",
    "        return self.__dict__\n",
    "\n",
    "    def add_gradient(self, model):\n",
    "        for name, param in model.named_parameters():\n",
    "            setattr(self, name, getattr(self, name) + param.grad.data.cpu())\n",
    "\n",
    "    def euclidian_distance(self, grad_current_agent):\n",
    "        \"\"\"Computes the relative euclidean distance of the flattened tensor between the current model and the global model\"\"\"\n",
    "        flattened_grad_selected = self.flatten(list(self.get_attributes().values()))\n",
    "        flattened_grad_current = self.flatten(list(grad_current_agent.get_attributes().values()))\n",
    "        return torch.dist(flattened_grad_selected, flattened_grad_current, 2) / torch.norm(flattened_grad_selected, 2)\n",
    "\n",
    "    def flatten(self, gradient_list):\n",
    "        \"\"\"Returns an aggregated tensor of all the gradients for one model\"\"\"\n",
    "        gradients = list(map(lambda g : torch.flatten(g), gradient_list))\n",
    "        return torch.cat(gradients, 0)\n",
    "\n",
    "\n",
    "def client_update(client_model, optimizer, train_loader, epoch=5):\n",
    "    \"\"\"Train a client_model on the train_loder data.\"\"\"\n",
    "    model_names = []\n",
    "    for name, param in client_model.named_parameters():\n",
    "        model_names.append(name)\n",
    "    gradient_stocker = GradientStocker(model_names)\n",
    "    for e in range(epoch):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output = client_model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            gradient_stocker.add_gradient(client_model)\n",
    "    return loss.item(), gradient_stocker\n",
    "\n",
    "\n",
    "def weighted_average_gradients(gradients, weights):\n",
    "    \"\"\"Compute the weighted average gradient.\"\"\"\n",
    "    weighted_averages = {}\n",
    "    for key in gradients[0].get_attributes().keys():\n",
    "        weighted_averages[key] = weighted_average_from_key(key, gradients, weights)\n",
    "    return weighted_averages\n",
    "\n",
    "def weighted_average_from_key(key, gradients, weights):\n",
    "    n = 0\n",
    "    d = 0 \n",
    "    for idx, g_dict in enumerate(gradients) :\n",
    "        n += g_dict.get_attributes()[key] * weights[idx]\n",
    "        d += weights[idx]\n",
    "    return n / d\n",
    "\n",
    "def compute_weight(alpha_prev, round, relative_distance, data_size, batch_size, distance_penalty, size_penalty):\n",
    "    \"\"\" Computes the weight alpha for round r \"\"\"\n",
    "    size_factor = (1 + size_penalty * math.floor(((round - 1) * batch_size) / data_size)) \n",
    "    distance_factor = distance_penalty * relative_distance\n",
    "    alpha = alpha_prev - size_factor * distance_factor \n",
    "    return max(0,alpha)\n",
    "\n",
    "def update_grad(model, gradient, alpha): \n",
    "    \"\"\" Update the gradient for all parameters\"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        param.data -= gradient[name].cuda() * alpha\n",
    "    return model \n",
    "\n",
    "def share_weight_erosion_model(shared_model, client_models):\n",
    "    \"\"\" Share the computed model with all agents\"\"\"\n",
    "    for model in client_models:\n",
    "        model.load_state_dict(shared_model.state_dict())\n",
    "\n",
    "def evaluate(global_model, data_loader):\n",
    "    \"\"\"Compute loss and accuracy of a model on a data_loader.\"\"\"\n",
    "    global_model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            output = global_model(data)\n",
    "            loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    loss /= len(data_loader.dataset)\n",
    "    acc = correct / len(data_loader.dataset)\n",
    "\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Run our model training using the Weight Erosion aggregation scheme === # \n",
    "\n",
    "def run_weight_erosion(train_loader, test_loader, num_clients, batch_size,\n",
    "                       selected_agent_index, num_rounds, epochs, distribution, distribution_name='distribution'):\n",
    "\n",
    "    distance_penalty = 0.1/num_clients\n",
    "    size_penalty = 2\n",
    "    dataPickle = []\n",
    "\n",
    "    print(\"=== Weight Erosion ===\")\n",
    "    np.set_printoptions(precision=3)\n",
    "    acc_best = 0\n",
    "    round_best = 0\n",
    "    weight_best = [0.1,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "    # Instantiate models and optimizers\n",
    "    shared_model = Net().cuda()\n",
    "    client_models = [Net().cuda() for _ in range(num_clients)]\n",
    "    for model in client_models:\n",
    "        model.load_state_dict(shared_model.state_dict())\n",
    "\n",
    "    opt = [optim.SGD(model.parameters(), lr=0.1) for model in client_models]\n",
    "\n",
    "    grad_vector = [None for _ in range(num_clients)]\n",
    "    weight_vector = np.ones(num_clients)\n",
    "\n",
    "    for r in range(num_rounds):\n",
    "\n",
    "        print('%d-th round' % r)\n",
    "\n",
    "        # client update\n",
    "        loss = np.zeros(num_clients)\n",
    "        for i in range(num_clients):\n",
    "            loss_tmp, grad_vector[i] = client_update(client_models[i], opt[i], train_loader[i], epoch=epochs)\n",
    "            loss[i] = loss_tmp\n",
    "            d_rel = grad_vector[selected_agent_index].euclidian_distance(grad_vector[i])\n",
    "            weight_vector[i] = compute_weight(weight_vector[i], r + 1, d_rel, len(train_loader[i]), batch_size, distance_penalty, size_penalty)\n",
    "\n",
    "\n",
    "        # Weight Erosion Scheme\n",
    "        weighted_mean_gradient = weighted_average_gradients(grad_vector, weight_vector)\n",
    "        shared_model = update_grad(shared_model, weighted_mean_gradient, 0.1)\n",
    "\n",
    "        # Share model to all agents\n",
    "        share_weight_erosion_model(shared_model, client_models)\n",
    "\n",
    "        # Evalutate on the agent's test set \n",
    "        test_loss, acc = evaluate(shared_model, test_loader)\n",
    "\n",
    "\n",
    "        print(f\"Weight : {weight_vector}\")\n",
    "        print(f\"Loss   : {loss}\")\n",
    "        print('Test loss %0.3g | Test acc: %0.3f \\n' % (test_loss, acc))\n",
    "        \n",
    "        # Keep the accuracy for each round \n",
    "        dataPickle.append([acc,test_loss,loss[selected_agent_index], sum(weight_vector)/num_clients])\n",
    "        \n",
    "        # Update the best accuracy \n",
    "        if acc > acc_best:\n",
    "            acc_best = acc\n",
    "            round_best = r+1\n",
    "            weight_best = weight_vector\n",
    "            \n",
    "    with open(Path.cwd()/'generated'/'pickles'/f'weight_erosion_{num_clients}_{distribution_name}.pickle', 'wb') as f:\n",
    "        pickle.dump(dataPickle, f)\n",
    "\n",
    "    return [acc_best, round_best, weight_best]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Run our model training using the Federated Average aggregation scheme === # \n",
    "\n",
    "def run_federated(train_loader, test_loader, num_clients,batch_size, \n",
    "                  selected_agent_index, num_rounds, epochs, distribution, distribution_name='distribution'):\n",
    "\n",
    "    print(\"=== Federated ===\")\n",
    "    np.set_printoptions(precision=3)\n",
    "    acc_best = 0\n",
    "    round_best = 0\n",
    "    weight_best = [0.1,0,0,0,0,0,0,0,0,0]\n",
    "    dataPickle = []\n",
    "\n",
    "    # Instantiate models and optimizers\n",
    "    shared_model = Net().cuda()\n",
    "    client_models = [Net().cuda() for _ in range(num_clients)]\n",
    "    for model in client_models:\n",
    "        model.load_state_dict(shared_model.state_dict())\n",
    "\n",
    "    opt = [optim.SGD(model.parameters(), lr=0.1) for model in client_models]\n",
    "\n",
    "    grad_vector = [None for _ in range(num_clients)]\n",
    "    weight_vector = np.ones(num_clients)\n",
    "\n",
    "    for r in range(num_rounds):\n",
    "\n",
    "        print('%d-th round' % r)\n",
    "\n",
    "        # client update\n",
    "        loss = np.zeros(num_clients)\n",
    "        for i in range(num_clients):\n",
    "            loss_tmp, grad_vector[i] = client_update(client_models[i], opt[i], train_loader[i], epoch=epochs)\n",
    "            loss[i] = loss_tmp\n",
    "            weight_vector[i] = 1/num_clients\n",
    "\n",
    "\n",
    "        # Weight Erosion Scheme\n",
    "        weighted_mean_gradient = weighted_average_gradients(grad_vector, weight_vector)\n",
    "        shared_model = update_grad(shared_model, weighted_mean_gradient, 0.1)\n",
    "\n",
    "        # Share model to all agents\n",
    "        share_weight_erosion_model(shared_model, client_models)\n",
    "\n",
    "        # Evalutate on the agent's test set\n",
    "        test_loss, acc = evaluate(shared_model, test_loader)\n",
    "\n",
    "        print(f\"Loss   : {loss}\")\n",
    "        print('Test loss %0.3g | Test acc: %0.3f\\n' % (test_loss, acc))\n",
    "        \n",
    "        # Keep the accuracy for each round \n",
    "        dataPickle.append([acc,test_loss,loss[selected_agent_index]])\n",
    "        \n",
    "        # Update the best accuracy \n",
    "        if acc > acc_best:\n",
    "            acc_best = acc\n",
    "            round_best = r+1\n",
    "            weight_best = weight_vector\n",
    "            \n",
    "    with open(Path.cwd()/'generated'/'pickles'/f'federated_{num_clients}_{distribution_name}.pickle', 'wb') as f:\n",
    "        pickle.dump(dataPickle, f)\n",
    "        \n",
    "    return [acc_best, round_best, weight_best]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Run our model training Locally === # \n",
    "\n",
    "def run_local(train_loader, test_loader, num_clients, batch_size, \n",
    "              selected_agent_index, num_rounds, epochs, distribution, distribution_name='distribution'):\n",
    "\n",
    "    print(\"=== Local ===\")\n",
    "    np.set_printoptions(precision=3)\n",
    "    dataPickle = []\n",
    "\n",
    "    # Instantiate models and optimizers\n",
    "    shared_model = Net().cuda()\n",
    "    client_models = [Net().cuda() for _ in range(num_clients)]\n",
    "    for model in client_models:\n",
    "        model.load_state_dict(shared_model.state_dict())\n",
    "\n",
    "    opt = [optim.SGD(model.parameters(), lr=0.1) for model in client_models]\n",
    "\n",
    "    grad_vector = 0\n",
    "    weight_vector = np.ones(num_clients)\n",
    "    \n",
    "    acc_best = 0\n",
    "    round_best = 0\n",
    "    weight_best = [1,0,0,0,0,0,0,0,0,0]\n",
    "    \n",
    "    print('%d-th Client' % selected_agent_index)\n",
    "    for r in range(num_rounds):\n",
    "        \n",
    "        print('%d-th round' % r)\n",
    "        \n",
    "        # client update\n",
    "        loss, grad_vector = client_update(client_models[selected_agent_index], opt[selected_agent_index], train_loader[selected_agent_index], epoch=epochs)\n",
    "\n",
    "        # Evalutate on the selected agent's test set \n",
    "        test_loss, acc = evaluate(client_models[selected_agent_index], test_loader)\n",
    "        \n",
    "        # Print the results \n",
    "        print(f\"Loss   : {loss}\")\n",
    "        print('Test loss %0.3g | Test acc: %0.3f\\n' % (test_loss, acc))\n",
    "        \n",
    "        # Keep the accuracy for each round \n",
    "        dataPickle.append([acc,test_loss,loss])\n",
    "        \n",
    "        # Update the best accuracy \n",
    "        if acc > acc_best:\n",
    "            acc_best = acc\n",
    "            round_best = r+1\n",
    "            \n",
    "    with open(Path.cwd()/'generated'/'pickles'/f'local_{num_clients}_{distribution_name}.pickle', 'wb') as f:\n",
    "        pickle.dump(dataPickle, f)\n",
    "\n",
    "    return [acc_best, round_best, weight_best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_iid_loader_distribution(num_clients,batch_size,distribution,selected_agent_index, validation_size=0.1):\n",
    "    traindata = datasets.MNIST('./data', train=True, download=True,transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "    testdata = datasets.MNIST('./data', train=False, download=True,transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "\n",
    "    target_labels = torch.stack([traindata.targets == i for i in range(10)])\n",
    "    target_labels_test = torch.stack([testdata.targets == i for i in range(10)])\n",
    "    target_labels_split = []\n",
    "    target_labels_split_test = []\n",
    "\n",
    "    #divide each target labels in small samples\n",
    "    target_label_division = 100 #need to check if with this number we have len(target_labels_split) = 10 * target_label_division\n",
    "    for i in range(10):\n",
    "        target_labels_data =torch.where(target_labels[i])[0]\n",
    "\n",
    "        target_labels_split += torch.split(target_labels_data, int((len(target_labels_data)) / (target_label_division-1)))\n",
    "        target_labels_split_test += torch.split(torch.where(target_labels_test[i])[0], int((len(torch.where(target_labels_test[i])[0]))))\n",
    "\n",
    "        target_labels_split = target_labels_split[:target_label_division*(i+1)] #remove when the split not givin you target_label_division samples but target_label_division +1 samples\n",
    "\n",
    "    #merge selected samples in each client\n",
    "    savedDistribution = distribution\n",
    "    distribution = [target_label_division * x / (max(num_clients,10)/10) for x in distribution]\n",
    "    samples_used = [0,0,0,0,0,0,0,0,0,0]\n",
    "    next_samples_used = [0,0,0,0,0,0,0,0,0,0]\n",
    "    split_client = []\n",
    "    test_data = torch.tensor([],dtype=torch.long)\n",
    "\n",
    "    for i in range(num_clients):\n",
    "        split_client.append(torch.tensor([],dtype=torch.long))\n",
    "        for n in range(10):\n",
    "            next_samples_used[n] = samples_used[n] + distribution[n]\n",
    "        distribution = distribution[1:] + distribution[:1] # shift to left\n",
    "        \n",
    "        for number in range(10):\n",
    "\n",
    "            #add data to test\n",
    "            if i == selected_agent_index and samples_used[number] < next_samples_used[number]:\n",
    "                sizeDataTest = int(savedDistribution[number] * len(target_labels_split_test[number]))\n",
    "                sizeDataTestLeft = len(target_labels_split_test[number]) - sizeDataTest\n",
    "                t1, t2 = torch.split(target_labels_split_test[number], [sizeDataTest,sizeDataTestLeft])\n",
    "\n",
    "                test_data = torch.cat((test_data, t1),0)\n",
    "\n",
    "            while samples_used[number] < next_samples_used[number]:\n",
    "                split_client[i] = torch.cat((split_client[i], target_labels_split[number*target_label_division+samples_used[number]]),0)\n",
    "                samples_used[number] += 1\n",
    "\n",
    "            if samples_used[number] > next_samples_used[number]:\n",
    "                samples_used[number] -= 1\n",
    "\n",
    "\n",
    "\n",
    "    traindata_split = [torch.utils.data.Subset(traindata, tl) for tl in split_client]\n",
    "    testdata_split = torch.utils.data.Subset(testdata, test_data)\n",
    "    train_loader = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in traindata_split]\n",
    "\n",
    "    #x_size = len(testdata_split)\n",
    "    #size_train = int(math.ceil(x_size * (1 - validation_size)))\n",
    "    #size_validation = int(math.floor(x_size * validation_size))\n",
    "    #test_set, validation_set = torch.utils.data.random_split(testdata_split, [size_train, size_validation])\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(testdata_split, batch_size=batch_size, shuffle=True)\n",
    "    #validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def get_iid_loader(num_clients,batch_size):\n",
    "      traindata = datasets.MNIST('./data', train=True, download=True,transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "      traindata_split = torch.utils.data.random_split(traindata, [int(traindata.data.shape[0] / num_clients) for _ in range(num_clients)])\n",
    "      train_loader = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in traindata_split]\n",
    "      test_loader = torch.utils.data.DataLoader(datasets.MNIST('./data', train=False, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,),(0.3081,))])), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "      return train_loader, test_loader\n",
    "\n",
    "def get_iid_loader_with_validation(num_clients, batch_size, validation_size=0.1):\n",
    "    traindata = datasets.MNIST('./data', train=True, download=True,transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "    testdata = datasets.MNIST('./data', train=False, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,),(0.3081,))]))\n",
    "    \n",
    "    traindata_split = torch.utils.data.random_split(traindata, [int(traindata.data.shape[0] / num_clients) for _ in range(num_clients)])\n",
    "    testdata_split = torch.utils.data.random_split(testdata, [int(testdata.data.shape[0] / num_clients) for _ in range(num_clients)])\n",
    "\n",
    "    train_loader = []\n",
    "    validation_loader = []\n",
    "\n",
    "    for x in traindata_split:\n",
    "      x_size = len(x)\n",
    "      size_train = int(math.ceil(x_size * (1 - validation_size)))\n",
    "      size_validation = int(math.floor(x_size * validation_size))\n",
    "      train_set, validation_set = torch.utils.data.random_split(x, [size_train, size_validation])\n",
    "\n",
    "      train_loader.append(torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True))\n",
    "      validation_loader.append(torch.utils.data.DataLoader(validation_set, batch_size=batch_size, shuffle=True))\n",
    "\n",
    "    test_loader = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in testdata_split]\n",
    "    \n",
    "\n",
    "    return train_loader, validation_loader, test_loader\n",
    "\n",
    "\n",
    "def get_non_IID_loader_digit_pairs(num_clients,batch_size):\n",
    "\n",
    "        traindata = datasets.MNIST('./data', train=True, download=True,transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "        testdata = datasets.MNIST('./data', train=False, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "        \n",
    "        train_target_labels = torch.stack([traindata.targets == i for i in range(10)])\n",
    "        test_target_labels = torch.stack([testdata.targets == i for i in range(10)])\n",
    "        \n",
    "        train_split_size = int(60000 / num_clients)\n",
    "        test_split_size = int(10000 / num_clients)\n",
    "\n",
    "        train_target_labels_split = []\n",
    "        test_target_labels_split = []\n",
    "\n",
    "        for i in range(num_clients):\n",
    "            train_target_labels_split += torch.split(torch.where(train_target_labels[(2 * i):(2 * (i + 1))].sum(0))[0][:train_split_size], train_split_size)\n",
    "            test_target_labels_split += torch.split(torch.where(test_target_labels[(2 * i):(2 * (i + 1))].sum(0))[0][:test_split_size], test_split_size)\n",
    "\n",
    "        traindata_split = [torch.utils.data.Subset(traindata, tl) for tl in train_target_labels_split]\n",
    "        testdata_split = [torch.utils.data.Subset(testdata, tl) for tl in test_target_labels_split]\n",
    "\n",
    "        train_loader = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in traindata_split]\n",
    "        test_loader = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in testdata_split]\n",
    "\n",
    "        return train_loader, test_loader\n",
    "\n",
    "# We want to give each agent 3 different digits \n",
    "# I'd say we do want to have all digitis at least once \n",
    "def generate_permutations(nb_agents=5, sample_size=3):\n",
    "      available_labels = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "      triplets = {}\n",
    "\n",
    "      valid = False \n",
    "      while not valid :\n",
    "        all_digits = []\n",
    "        for i in range(nb_agents):\n",
    "          triplets[i] = np.random.choice(available_labels,sample_size,replace=False)\n",
    "          all_digits.extend(triplets[i])\n",
    "        valid = len(np.unique(all_digits)) == len(available_labels)\n",
    "      return triplets\n",
    " \n",
    "    \n",
    "def get_non_IID_loader_digit_trios(num_clients,batch_size):\n",
    "\n",
    "        traindata = datasets.MNIST('./data', train=True, download=True,transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "        testdata = datasets.MNIST('./data', train=False, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "        \n",
    "        train_target_labels = torch.stack([traindata.targets == i for i in range(10)])\n",
    "        test_target_labels = torch.stack([testdata.targets == i for i in range(10)])\n",
    "        \n",
    "        train_split_size = int(60000 / num_clients)\n",
    "        test_split_size = int(10000 / num_clients)\n",
    "\n",
    "        train_target_labels_split = []\n",
    "        test_target_labels_split = []\n",
    "\n",
    "        triplets = generate_permutations(num_clients)\n",
    "\n",
    "        for i in range(num_clients):\n",
    "            i_labels = triplets[i]\n",
    "            print(f\"Agent {i} is assigned labels {i_labels}\")\n",
    "            train_target_labels_split += torch.split(torch.where(train_target_labels[i_labels].sum(0))[0][:train_split_size], train_split_size)\n",
    "            test_target_labels_split += torch.split(torch.where(test_target_labels[i_labels].sum(0))[0][:test_split_size], test_split_size)\n",
    "\n",
    "        traindata_split = [torch.utils.data.Subset(traindata, tl) for tl in train_target_labels_split]\n",
    "        testdata_split = [torch.utils.data.Subset(testdata, tl) for tl in test_target_labels_split]\n",
    "\n",
    "        train_loader = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in traindata_split]\n",
    "        test_loader = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in testdata_split]\n",
    "\n",
    "        return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Run the Benchmarking === #  \n",
    "\n",
    "# === parameters for the aggregation Schemes === #\n",
    "\n",
    "selected_agent_index = 0\n",
    "num_rounds = 30\n",
    "epochs = 1\n",
    "\n",
    "# === parameters for training and testing === #\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "# === Benchmarking Parameters === #\n",
    "\n",
    "# These are all distributions and number of clients on which we are running our algorithms \n",
    "distributions = {'A' : [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
    "                 'B' : [0, 0, 0, 0, 0.2, 0.6, 0.2, 0, 0, 0],\n",
    "                 'C' : [0.25, 0.25, 0.25, 0.25, 0, 0, 0, 0, 0, 0],\n",
    "                 'D' : [0, 0, 0, 0.4, 0.1, 0, 0.1, 0.4, 0, 0],\n",
    "                 'E' : [0, 0, 0, 0.1, 0.2, 0.4, 0.2, 0.1, 0, 0],\n",
    "                 'F' : [0, 0, 0.1, 0.1, 0.2, 0.2, 0.2, 0.1, 0.1, 0],\n",
    "                 'G' : [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01 ]}\n",
    "\n",
    "clients = [10, 20, 50, 100]\n",
    "\n",
    "# We keep track of our computation's carbon footprint\n",
    "cumulator = base.Cumulator()\n",
    "cumulator.on()\n",
    "\n",
    "for name, distribution in distributions.items():\n",
    "    for num_clients in clients:\n",
    "        print(' - Number Client %0.3g, distribution %s: %s' % (num_clients, name, distribution))\n",
    "        train_loader, test_loader = get_non_iid_loader_distribution(num_clients,batch_size,distribution,selected_agent_index)\n",
    "\n",
    "        dataPickle = []\n",
    "        \n",
    "        # === Run with Weight Erosion aggregation Scheme \n",
    "        dataPickle.append(run_weight_erosion(train_loader, test_loader, num_clients, batch_size, selected_agent_index, num_rounds, epochs, distribution, distribution_name=name))\n",
    "\n",
    "        # === Run Federated Learning aggregation Scheme \n",
    "        dataPickle.append(run_federated(train_loader, test_loader, num_clients, batch_size, selected_agent_index, num_rounds, epochs, distribution, distribution_name=name))\n",
    "\n",
    "        # === Run Local Training \n",
    "        dataPickle.append(run_local(train_loader, test_loader, num_clients,batch_size, selected_agent_index, num_rounds, epochs, distribution, distribution_name=name))\n",
    "        \n",
    "        # === Store the results as pickles \n",
    "        with open(Path.cwd()/'generated'/'pickles'/f'{num_clients}_{name}.pickle', 'wb') as f:\n",
    "            pickle.dump(dataPickle, f)\n",
    "            \n",
    "cumulator.off()\n",
    "print(f'The total carbon footprint generated by this benchmark is : {cumulator.total_carbon_footprint()} gCO2eq')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# To Remove \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Plots accuracy for each algorithms for w.r.t. the number of rounds\n",
    "def plot(nb_round, WE_result, FedAvg_result, local_result, weight_avg, title, file_name):\n",
    "    plt.figure(num=None, figsize=(9, 7), facecolor='w', edgecolor='k')\n",
    "    plt.plot(nb_round, WE_result, '-xb', label='WE')\n",
    "    plt.plot(nb_round, FedAvg_result, '-og', label=\"FedAvg\")\n",
    "    plt.plot(nb_round, local_result, '-+k', label=\"local\")\n",
    "    plt.plot(nb_round, weight_avg, color='r', linestyle='dashed', label='Average weight')\n",
    "    plt.title(title)\n",
    "    ax = plt.gca()\n",
    "    ax.legend(loc=3)\n",
    "    ax.set_xlabel(\"Communication round\", fontsize=12)\n",
    "    ax.set_ylabel(\"Test accuracy / Agents Average weight\", fontsize=12)\n",
    "    plt.savefig(Path.cwd()/'generated'/'plots'/f'{file_name}.png')\n",
    "    plt.show()\n",
    "    \n",
    "def format_for_plot(data, index=0):\n",
    "    return np.array([d[index] for d in data])\n",
    "pickles_folder = Path.cwd()/'generated'/'pickles'\n",
    "\n",
    "\n",
    "def plot_pickled_results(distributions, nb_agents):\n",
    "    for d_name in distributions.keys():\n",
    "        for n in nb_agents:\n",
    "            with open(pickles_folder/f'federated_{n}_{d_name}_m.pickle', 'rb') as f:\n",
    "                data_fed = pickle.load(f)\n",
    "            with open(pickles_folder/f'weight_erosion_{n}_{d_name}.pickle', 'rb') as f:\n",
    "                data_we = pickle.load(f)\n",
    "            with open(pickles_folder/f'local_{n}_{d_name}.pickle', 'rb') as f:\n",
    "                data_local = pickle.load(f)\n",
    "\n",
    "            nb_round = np.arange(30)\n",
    "            acc_fed_plot = format_for_plot(data_fed)\n",
    "            acc_we_plot = format_for_plot(data_we)\n",
    "            weight_avg = format_for_plot(data_we, 3)\n",
    "            acc_loc_plot = format_for_plot(data_local)\n",
    "\n",
    "\n",
    "            plot(nb_round=nb_round, WE_result=acc_we_plot, FedAvg_result=acc_fed_plot, local_result=acc_loc_plot, \n",
    "                 weight_avg=weight_avg, title=f'Test accuracies for distribution {d_name} with {n} agents', \n",
    "                 file_name=f'results_distribution_{d_name}_{n}_agents')\n",
    "\n",
    "def plot_pickled_results_old_format():\n",
    "    # DEPRECATED, ONLY AVAILABLE FOR PREVIOUSLY COMPUTED .pickle\n",
    "    nb_agents = [10, 20, 50, 100]\n",
    "    distribs = [0, 0.2, 0.4, 0.6]\n",
    "    for d in distribs:\n",
    "        for na in nb_agents:\n",
    "            with open(pickles_folder/f'federated_{na}-{d}_m.pickle', 'rb') as f:\n",
    "                data_fed = pickle.load(f)\n",
    "            with open(pickles_folder/f'weightErosion_{na}-{d}_m.pickle', 'rb') as f:\n",
    "                data_we = pickle.load(f)\n",
    "            with open(pickles_folder/f'local_{na}-{d}_m.pickle', 'rb') as f:\n",
    "                data_local = pickle.load(f)\n",
    "\n",
    "            nb_round = np.arange(30)\n",
    "            acc_fed_plot = format_for_plot(data_fed)\n",
    "            acc_we_plot = format_for_plot(data_we)\n",
    "            weight_avg = format_for_plot(data_we, 3)\n",
    "            acc_loc_plot = np.array([d[0] for d in data_local]*30)\n",
    "\n",
    "\n",
    "            plot(nb_round=nb_round, WE_result=acc_we_plot, FedAvg_result=acc_fed_plot, local_result=acc_loc_plot, weight_avg=weight_avg, title=f'plot_{na}-{d}', file_name=f'plot_{na}-{d}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=== Non-IID case \n",
    "#=== parameters for Schemes\n",
    "selected_agent_index = 0\n",
    "num_rounds = 20\n",
    "epochs = 1\n",
    "\n",
    "#=== parameters for training and testing\n",
    "num_clients = 5\n",
    "batch_size = 32\n",
    "homogeneity = False\n",
    "\n",
    "#train_loader, test_loader = get_specific_non_IID_loader(num_clients,batch_size,homogeneity) \n",
    "train_loader, test_loader = get_non_IID_loader_digit_trios(num_clients,batch_size)\n",
    "#cumulator not done yet\n",
    "cumulator = base.Cumulator()\n",
    "cumulator.on()\n",
    "\n",
    "run_weight_erosion_non_IID(train_loader,test_loader,num_clients,batch_size,selected_agent_index,num_rounds,epochs)\n",
    "\n",
    "cumulator.off()\n",
    "print('The total carbon footprint for these computations is : ',cumulator.total_carbon_footprint())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
