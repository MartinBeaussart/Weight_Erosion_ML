{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sESj8k1APL8E"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.fc1 = nn.Linear(2048, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "class GradientStocker:\n",
    "    def __init__(self, model_names):\n",
    "        for item in model_names:\n",
    "            setattr(self, item, 0)\n",
    "\n",
    "    def get_attributes(self):\n",
    "        return self.__dict__\n",
    "\n",
    "    def add_gradient(self, model):\n",
    "        for name, param in model.named_parameters():\n",
    "            setattr(self, name, getattr(self, name) + param.grad.data.cpu())\n",
    "\n",
    "    def euclidian_distance(self, grad_current_agent):\n",
    "        \"\"\"Computes the relative euclidean distance of the flattened tensor between the current model and the global model\"\"\"\n",
    "        flattened_grad_selected = self.flatten(list(self.get_attributes().values()))\n",
    "        flattened_grad_current = self.flatten(list(grad_current_agent.get_attributes().values()))\n",
    "        return torch.dist(flattened_grad_selected, flattened_grad_current, 2) / torch.norm(flattened_grad_selected, 2)\n",
    "\n",
    "    def flatten(self, gradient_list):\n",
    "        \"\"\"Returns an aggregated tensor of all the gradients for one model\"\"\"\n",
    "        gradients = list(map(lambda g : torch.flatten(g), gradient_list))\n",
    "        return torch.cat(gradients, 0)\n",
    "\n",
    "\n",
    "def client_update(client_model, optimizer, train_loader, epoch=5):\n",
    "    \"\"\"Train a client_model on the train_loder data.\"\"\"\n",
    "    model_names = []\n",
    "    for name, param in client_model.named_parameters():\n",
    "        model_names.append(name)\n",
    "    gradient_stocker = GradientStocker(model_names)\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output = client_model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            gradient_stocker.add_gradient(client_model)\n",
    "    return loss.item(), gradient_stocker\n",
    "\n",
    "\n",
    "def weighted_average_gradients(gradients, weights):\n",
    "    \"\"\"Compute the weighted average gradient.\"\"\"\n",
    "    weighted_averages = {}\n",
    "    for key in gradients[0].get_attributes().keys():\n",
    "        weighted_averages[key] = weighted_average_from_key(key, gradients, weights)\n",
    "    return weighted_averages\n",
    "\n",
    "def weighted_average_from_key(key, gradients, weights):\n",
    "    n = 0\n",
    "    d = 0 \n",
    "    for idx, g_dict in enumerate(gradients) :\n",
    "        n += g_dict.get_attributes()[key] * weights[idx]\n",
    "        d += weights[idx]\n",
    "    return n / d\n",
    "\n",
    "def compute_weight(alpha_prev, round, relative_distance, data_size, batch_size, distance_penalty, size_penalty):\n",
    "    \"\"\"Computes the weight alpha for round r\"\"\"\n",
    "    size_factor = (1 + size_penalty * math.floor(((round - 1) * batch_size) / data_size)) \n",
    "    distance_factor = distance_penalty * relative_distance\n",
    "    alpha = alpha_prev - size_factor * distance_factor \n",
    "    return max(0,alpha)\n",
    "\n",
    "def update_grad(model, gradient, alpha): \n",
    "    for name, param in model.named_parameters():\n",
    "        param.data -= gradient[name].cuda() * alpha\n",
    "    return model \n",
    "\n",
    "def share_weight_erosion_model(shared_model, client_models):\n",
    "    for model in client_models:\n",
    "        model.load_state_dict(shared_model.state_dict())\n",
    "\n",
    "def evaluate(global_model, data_loader):\n",
    "    \"\"\"Compute loss and accuracy of a model on a data_loader.\"\"\"\n",
    "    global_model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            output = global_model(data)\n",
    "            loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    loss /= len(data_loader.dataset)\n",
    "    acc = correct / len(data_loader.dataset)\n",
    "\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TfD7XCJky_Kv",
    "outputId": "026a59fd-8cf9-47b8-f21f-352d01d63f55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3nba1ce4fcLh"
   },
   "outputs": [],
   "source": [
    "distance_penalty = 0.05\n",
    "size_penalty = 2\n",
    "\n",
    "def runWeightErosion(train_loader,test_loader,num_clients,batch_size,selected_agent_index,num_rounds,epochs):\n",
    "\n",
    "    print(\"=== Weight Erosion ===\")\n",
    "    np.set_printoptions(precision=3)\n",
    "\n",
    "    # Instantiate models and optimizers\n",
    "    shared_model = Net().cuda()\n",
    "    client_models = [Net().cuda() for _ in range(num_clients)]\n",
    "    for model in client_models:\n",
    "        model.load_state_dict(shared_model.state_dict())\n",
    "\n",
    "    opt = [optim.SGD(model.parameters(), lr=0.1) for model in client_models]\n",
    "\n",
    "    grad_vector = [None for _ in range(num_clients)]\n",
    "    weight_vector = np.ones(num_clients)\n",
    "\n",
    "    for r in range(num_rounds):\n",
    "\n",
    "        print('%d-th round' % r)\n",
    "\n",
    "        # client update\n",
    "        loss = np.zeros(num_clients)\n",
    "        for i in range(num_clients):\n",
    "            loss_tmp, grad_vector[i] = client_update(client_models[i], opt[i], train_loader[i], epoch=epochs)\n",
    "            loss[i] = loss_tmp\n",
    "            d_rel = grad_vector[selected_agent_index].euclidian_distance(grad_vector[i])\n",
    "            weight_vector[i] = compute_weight(weight_vector[i], r + 1, d_rel, len(train_loader[i]), batch_size, distance_penalty, size_penalty)\n",
    "\n",
    "\n",
    "        # Weight Erosion Scheme\n",
    "        weighted_mean_gradient = weighted_average_gradients(grad_vector, weight_vector)\n",
    "        shared_model = update_grad(shared_model, weighted_mean_gradient, 0.1)\n",
    "\n",
    "        # Share model to all agents\n",
    "        share_weight_erosion_model(shared_model, client_models)\n",
    "\n",
    "        # Evalutate on the global test set (for now)\n",
    "        test_loss, acc = evaluate(shared_model, test_loader)\n",
    "\n",
    "\n",
    "        print(f\"Weight : {weight_vector}\")\n",
    "        print(f\"Loss   : {loss}\")\n",
    "        print('Test loss %0.3g | Test acc: %0.3f \\n' % (test_loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gxNNVhRh6N_R"
   },
   "outputs": [],
   "source": [
    "distance_penalty = 0.05\n",
    "size_penalty = 2\n",
    "\n",
    "def run_weight_erosion_non_IID(train_loader,test_loader,num_clients,batch_size,selected_agent_index,num_rounds,epochs):\n",
    "\n",
    "    print(\"=== Weight Erosion Non-IID ===\")\n",
    "    np.set_printoptions(precision=3)\n",
    "\n",
    "    # Instantiate models and optimizers\n",
    "    shared_model = Net().cuda()\n",
    "    client_models = [Net().cuda() for _ in range(num_clients)]\n",
    "    for model in client_models:\n",
    "        model.load_state_dict(shared_model.state_dict())\n",
    "\n",
    "    opt = [optim.SGD(model.parameters(), lr=0.1) for model in client_models]\n",
    "\n",
    "    grad_vector = [None for _ in range(num_clients)]\n",
    "    weight_vector = np.ones(num_clients)\n",
    "\n",
    "    for r in range(num_rounds):\n",
    "\n",
    "        print('%d-th round' % r)\n",
    "\n",
    "        # client update\n",
    "        loss = np.zeros(num_clients)\n",
    "        for i in range(num_clients):\n",
    "            loss_tmp, grad_vector[i] = client_update(client_models[i], opt[i], train_loader[i], epoch=epochs)\n",
    "            loss[i] = loss_tmp\n",
    "            d_rel = grad_vector[selected_agent_index].euclidian_distance(grad_vector[i])\n",
    "            weight_vector[i] = compute_weight(weight_vector[i], r + 1, d_rel, len(train_loader[i]), batch_size, distance_penalty, size_penalty)\n",
    "\n",
    "\n",
    "        # Weight Erosion Scheme\n",
    "        weighted_mean_gradient = weighted_average_gradients(grad_vector, weight_vector)\n",
    "        shared_model = update_grad(shared_model, weighted_mean_gradient, 0.1)\n",
    "\n",
    "        # Share model to all agents\n",
    "        share_weight_erosion_model(shared_model, client_models)\n",
    "\n",
    "        # Evalutate on the global test set\n",
    "        test_acc = np.zeros(num_clients)\n",
    "        for idx in range(num_clients):\n",
    "          test_loss, test_acc[idx] = evaluate(shared_model, test_loader[idx])\n",
    "\n",
    "        print(f\"Weight : {weight_vector}\")\n",
    "        print(f\"Loss   : {loss}\")\n",
    "        np.set_printoptions(precision=5)\n",
    "        print(f\"Test acc : {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "e_1BLW_UfcLh"
   },
   "outputs": [],
   "source": [
    "def runFederated(train_loader,test_loader,num_clients,batch_size,selected_agent_index,num_rounds,epochs):\n",
    "\n",
    "    print(\"=== Federated ===\")\n",
    "    np.set_printoptions(precision=3)\n",
    "\n",
    "    # Instantiate models and optimizers\n",
    "    shared_model = Net().cuda()\n",
    "    client_models = [Net().cuda() for _ in range(num_clients)]\n",
    "    for model in client_models:\n",
    "        model.load_state_dict(shared_model.state_dict())\n",
    "\n",
    "    opt = [optim.SGD(model.parameters(), lr=0.1) for model in client_models]\n",
    "\n",
    "    grad_vector = [None for _ in range(num_clients)]\n",
    "    weight_vector = np.ones(num_clients)\n",
    "\n",
    "    for r in range(num_rounds):\n",
    "\n",
    "        print('%d-th round' % r)\n",
    "\n",
    "        # client update\n",
    "        loss = np.zeros(num_clients)\n",
    "        for i in range(num_clients):\n",
    "            loss_tmp, grad_vector[i] = client_update(client_models[i], opt[i], train_loader[i], epoch=epochs)\n",
    "            loss[i] = loss_tmp\n",
    "            weight_vector[i] = 1/num_clients\n",
    "\n",
    "\n",
    "        # Weight Erosion Scheme\n",
    "        weighted_mean_gradient = weighted_average_gradients(grad_vector, weight_vector)\n",
    "        shared_model = update_grad(shared_model, weighted_mean_gradient, 0.1)\n",
    "\n",
    "        # Share model to all agents\n",
    "        share_weight_erosion_model(shared_model, client_models)\n",
    "\n",
    "        # Evalutate on the global test set (for now)\n",
    "        test_loss, acc = evaluate(shared_model, test_loader)\n",
    "\n",
    "        print(f\"Loss   : {loss}\")\n",
    "        print('Test loss %0.3g | Test acc: %0.3f\\n' % (test_loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "GSfAo6jifcLh"
   },
   "outputs": [],
   "source": [
    "def runLocal(train_loader,test_loader,num_clients,batch_size,selected_agent_index,epochs):\n",
    "\n",
    "    print(\"=== Local ===\")\n",
    "    np.set_printoptions(precision=3)\n",
    "\n",
    "    # Instantiate models and optimizers\n",
    "    shared_model = Net().cuda()\n",
    "    client_models = [Net().cuda() for _ in range(num_clients)]\n",
    "    for model in client_models:\n",
    "        model.load_state_dict(shared_model.state_dict())\n",
    "\n",
    "    opt = [optim.SGD(model.parameters(), lr=0.1) for model in client_models]\n",
    "\n",
    "    grad_vector = 0\n",
    "    weight_vector = np.ones(num_clients)\n",
    "\n",
    "    # client update\n",
    "    loss = 0\n",
    "\n",
    "    print('%d-th Client' % selected_agent_index)\n",
    "    loss_tmp, grad_vector = client_update(client_models[selected_agent_index], opt[selected_agent_index], train_loader[selected_agent_index], epoch=epochs)\n",
    "    loss = loss_tmp\n",
    "\n",
    "    # Evalutate on the global test set (for now)\n",
    "    test_loss, acc = evaluate(client_models[selected_agent_index], test_loader)\n",
    "\n",
    "    print(f\"Loss   : {loss}\")\n",
    "    print('Test loss %0.3g | Test acc: %0.3f\\n' % (test_loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9FnLxzbifcLh"
   },
   "outputs": [],
   "source": [
    "def get_iid_loader(num_clients,batch_size):\n",
    "    if homogeneity:\n",
    "        traindata = datasets.MNIST('./data', train=True, download=True,transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "        traindata_split = torch.utils.data.random_split(traindata, [int(traindata.data.shape[0] / num_clients) for _ in range(num_clients)])\n",
    "        train_loader = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in traindata_split]\n",
    "        test_loader = torch.utils.data.DataLoader(datasets.MNIST('./data', train=False, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,),(0.3081,))])), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        return train_loader, test_loader\n",
    "\n",
    "def get_non_iid_loader_distribution(num_clients,batch_size,distribution,selected_agent_index):\n",
    "    traindata = datasets.MNIST('./data', train=True, download=True,transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "    testdata = datasets.MNIST('./data', train=False, download=True,transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "\n",
    "    target_labels = torch.stack([traindata.targets == i for i in range(10)])\n",
    "    target_labels_test = torch.stack([testdata.targets == i for i in range(10)])\n",
    "    target_labels_split = []\n",
    "    target_labels_split_test = []\n",
    "\n",
    "    #divide each target labels in small samples\n",
    "    target_label_division = 100 #need to check if with this number we have len(target_labels_split) = 10 * target_label_division\n",
    "    for i in range(10):\n",
    "        target_labels_data =torch.where(target_labels[i])[0]\n",
    "\n",
    "        target_labels_split += torch.split(target_labels_data, int((len(target_labels_data)) / (target_label_division-1)))\n",
    "        target_labels_split_test += torch.split(torch.where(target_labels_test[i%10])[0], int((len(torch.where(target_labels_test[i])[0]))))\n",
    "\n",
    "        target_labels_split = target_labels_split[:target_label_division*(i+1)] #remove when the split not givin you target_label_division samples but target_label_division +1 samples\n",
    "\n",
    "    #merge selected samples in each client\n",
    "    distribution = [target_label_division * x / (max(num_clients,10)/10) for x in distribution]\n",
    "    samples_used = [0,0,0,0,0,0,0,0,0,0]\n",
    "    next_samples_used = [0,0,0,0,0,0,0,0,0,0]\n",
    "    split_client = []\n",
    "    test_data = torch.tensor([],dtype=torch.long)\n",
    "\n",
    "    for i in range(num_clients):\n",
    "        split_client.append(torch.tensor([],dtype=torch.long))\n",
    "        for n in range(10):\n",
    "            next_samples_used[n] = samples_used[n] + distribution[n]\n",
    "        distribution = distribution[1:] + distribution[:1] # shift to left\n",
    "\n",
    "        for number in range(10):\n",
    "            if i == selected_agent_index and samples_used[number] < next_samples_used[number]:\n",
    "                test_data = torch.cat((test_data, target_labels_split_test[number]),0)\n",
    "\n",
    "            while samples_used[number] < next_samples_used[number]:\n",
    "                split_client[i] = torch.cat((split_client[i], target_labels_split[number*target_label_division+samples_used[number]]),0)\n",
    "                samples_used[number] += 1\n",
    "\n",
    "            if samples_used[number] > next_samples_used[number]:\n",
    "                samples_used[number] -= 1\n",
    "\n",
    "    traindata_split = [torch.utils.data.Subset(traindata, tl) for tl in split_client]\n",
    "    testdata_split = torch.utils.data.Subset(testdata, test_data)\n",
    "    train_loader = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in traindata_split]\n",
    "    test_loader = torch.utils.data.DataLoader(testdata_split, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def get_specific_non_IID_loader(num_clients,batch_size,homogeneity):\n",
    "\n",
    "    traindata = datasets.MNIST('./data', train=True, download=True,transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "\n",
    "    target_labels = torch.stack([traindata.targets == i for i in range(10)])\n",
    "\n",
    "    target_labels_split = []\n",
    "    split_size = int(60000 / num_clients)\n",
    "\n",
    "    for i in range(num_clients):\n",
    "        target_labels_split += torch.split(torch.where(target_labels[(2 * i):(2 * (i + 1))].sum(0))[0][:split_size], split_size)\n",
    "\n",
    "    traindata_split = [torch.utils.data.Subset(traindata, tl) for tl in target_labels_split]\n",
    "    train_loader = []\n",
    "    test_loader = []\n",
    "    for x in traindata_split:\n",
    "      x_size = len(x)\n",
    "      size_train = int(math.ceil(x_size * 0.7))\n",
    "      size_test = int(math.floor(x_size * 0.3))\n",
    "      #print(x_size == size_train + size_test, size_train, size_test)\n",
    "      train_set, test_set = torch.utils.data.random_split(x, [size_train, size_test])\n",
    "      train_loader.append(torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True))\n",
    "      test_loader.append(torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True))\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def get_non_IID_loader_digit_pairs(num_clients,batch_size,homogeneity):\n",
    "\n",
    "        traindata = datasets.MNIST('./data', train=True, download=True,transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "        testdata = datasets.MNIST('./data', train=False, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "        \n",
    "        train_target_labels = torch.stack([traindata.targets == i for i in range(10)])\n",
    "        test_target_labels = torch.stack([testdata.targets == i for i in range(10)])\n",
    "        \n",
    "        train_split_size = int(60000 / num_clients)\n",
    "        test_split_size = int(10000 / num_clients)\n",
    "\n",
    "        train_target_labels_split = []\n",
    "        test_target_labels_split = []\n",
    "\n",
    "        for i in range(num_clients):\n",
    "            train_target_labels_split += torch.split(torch.where(train_target_labels[(2 * i):(2 * (i + 1))].sum(0))[0][:train_split_size], train_split_size)\n",
    "            test_target_labels_split += torch.split(torch.where(test_target_labels[(2 * i):(2 * (i + 1))].sum(0))[0][:test_split_size], test_split_size)\n",
    "\n",
    "        traindata_split = [torch.utils.data.Subset(traindata, tl) for tl in train_target_labels_split]\n",
    "        testdata_split = [torch.utils.data.Subset(testdata, tl) for tl in test_target_labels_split]\n",
    "\n",
    "        train_loader = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in traindata_split]\n",
    "        test_loader = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in testdata_split]\n",
    "\n",
    "        return train_loader, test_loader\n",
    "\n",
    "def get_non_IID_loader_digit_trios(num_clients,batch_size,homogeneity):\n",
    "\n",
    "        traindata = datasets.MNIST('./data', train=True, download=True,transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "        testdata = datasets.MNIST('./data', train=False, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "        \n",
    "        train_target_labels = torch.stack([traindata.targets == i for i in range(10)])\n",
    "        test_target_labels = torch.stack([testdata.targets == i for i in range(10)])\n",
    "        \n",
    "        train_split_size = int(60000 / num_clients)\n",
    "        test_split_size = int(10000 / num_clients)\n",
    "\n",
    "        train_target_labels_split = []\n",
    "        test_target_labels_split = []\n",
    "\n",
    "        triplets = generate_permutations(num_clients)\n",
    "\n",
    "        for i in range(num_clients):\n",
    "            i_labels = triplets[i]\n",
    "            print(f\"Agent {i} is assigned labels {i_labels}\")\n",
    "            train_target_labels_split += torch.split(torch.where(train_target_labels[i_labels].sum(0))[0][:train_split_size], train_split_size)\n",
    "            test_target_labels_split += torch.split(torch.where(test_target_labels[i_labels].sum(0))[0][:test_split_size], test_split_size)\n",
    "\n",
    "        traindata_split = [torch.utils.data.Subset(traindata, tl) for tl in train_target_labels_split]\n",
    "        testdata_split = [torch.utils.data.Subset(testdata, tl) for tl in test_target_labels_split]\n",
    "\n",
    "        train_loader = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in traindata_split]\n",
    "        test_loader = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in testdata_split]\n",
    "\n",
    "        return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "x4Bwmrpo3gsj"
   },
   "outputs": [],
   "source": [
    "# We want to give each agent 3 different digits \n",
    "# I'd say we do want to have all digitis at least once \n",
    "def generate_permutations(nb_agents=5, sample_size=3):\n",
    "  available_labels = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "  triplets = {}\n",
    "\n",
    "  valid = False \n",
    "  while not valid :\n",
    "    all_digits = []\n",
    "    for i in range(nb_agents):\n",
    "      triplets[i] = np.random.choice(available_labels,sample_size,replace=False)\n",
    "      all_digits.extend(triplets[i])\n",
    "    valid = len(np.unique(all_digits)) == len(available_labels)\n",
    "  return triplets\n",
    " \n",
    "\n",
    "      \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7fxXxCp4fcLh",
    "outputId": "37b549de-8ace-4ab4-be0a-887eb14be89d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: cumulator in /usr/lib/python3.8/site-packages (0.0.7)\n",
      "=== Local ===\n",
      "0-th Client\n",
      "Loss   : 0.0\n",
      "Test loss 0.0306 | Test acc: 0.991\n",
      "\n",
      "0.6004105812311172\n"
     ]
    }
   ],
   "source": [
    "!pip install cumulator\n",
    "from cumulator import base\n",
    "\n",
    "#=== IID Case \n",
    "\n",
    "#=== parameters for Schemes\n",
    "selected_agent_index = 0\n",
    "num_rounds = 10\n",
    "epochs = 1\n",
    "\n",
    "#=== parameters for training and testing\n",
    "num_clients = 10 #if num_clients < 10, sum(distribution) should be = 10/num_clients with max 1 at each index\n",
    "batch_size = 32\n",
    "homogeneity = False\n",
    "distribution = [0,0,0,0.25,0.5,0.25,0,0,0,0]\n",
    "\n",
    "\n",
    "train_loader, test_loader = get_non_iid_loader_distribution(num_clients,batch_size,distribution,selected_agent_index) #lot of change needed\n",
    "\n",
    "#cumulator not done yet\n",
    "cumulator = base.Cumulator()\n",
    "cumulator.on()\n",
    "\n",
    "#runWeightErosion(train_loader,test_loader,num_clients,batch_size,selected_agent_index,num_rounds,epochs)\n",
    "#runFederated(train_loader,test_loader,num_clients,batch_size,selected_agent_index,num_rounds,epochs)\n",
    "runLocal(train_loader,test_loader,num_clients,batch_size,selected_agent_index,num_rounds*epochs)\n",
    "\n",
    "cumulator.off()\n",
    "dontknow = cumulator.computation_costs()\n",
    "print(dontknow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1N8Eb-4mhHZp",
    "outputId": "7274061d-0241-4e12-b428-8e4879e4793a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0 is assigned labels [2 5 3]\n",
      "Agent 1 is assigned labels [4 8 9]\n",
      "Agent 2 is assigned labels [7 0 8]\n",
      "Agent 3 is assigned labels [9 1 8]\n",
      "Agent 4 is assigned labels [7 9 6]\n",
      "=== Weight Erosion Non-IID ===\n",
      "0-th round\n",
      "Weight : [1.    0.933 0.94  0.936 0.936]\n",
      "Loss   : [0.028 0.043 0.04  0.074 0.013]\n",
      "Test acc : [0.     0.653  0.3345 0.6225 0.3395]\n",
      "1-th round\n",
      "Weight : [1.      0.86565 0.88156 0.87467 0.8754 ]\n",
      "Loss   : [0.00299 0.00686 0.00049 0.15042 0.00301]\n",
      "Test acc : [0.0415 0.668  0.5885 0.6665 0.5775]\n",
      "2-th round\n",
      "Weight : [1.      0.80024 0.82368 0.81457 0.81449]\n",
      "Loss   : [0.03506 0.09882 0.00092 0.03042 0.00655]\n",
      "Test acc : [0.312  0.726  0.809  0.9105 0.758 ]\n",
      "3-th round\n",
      "Weight : [1.      0.73307 0.76304 0.75045 0.75146]\n",
      "Loss   : [0.00168 0.08008 0.008   0.00095 0.00497]\n",
      "Test acc : [0.6085 0.7665 0.89   0.9575 0.857 ]\n",
      "4-th round\n",
      "Weight : [1.      0.66029 0.70133 0.68202 0.68258]\n",
      "Loss   : [0.01779 0.05878 0.01646 0.00124 0.00314]\n",
      "Test acc : [0.722  0.857  0.921  0.9775 0.867 ]\n",
      "5-th round\n",
      "Weight : [1.      0.58992 0.63759 0.61729 0.61396]\n",
      "Loss   : [0.02174 0.02055 0.00125 0.00547 0.0014 ]\n",
      "Test acc : [0.8685 0.87   0.9305 0.9805 0.8815]\n",
      "6-th round\n",
      "Weight : [1.      0.51686 0.57297 0.55003 0.54602]\n",
      "Loss   : [1.68063e-02 6.21753e-03 5.22804e-05 6.60963e-04 5.39519e-04]\n",
      "Test acc : [0.909  0.9095 0.944  0.98   0.8865]\n",
      "7-th round\n",
      "Weight : [1.      0.42746 0.50582 0.48136 0.47515]\n",
      "Loss   : [0.00025 0.0712  0.00882 0.00027 0.00239]\n",
      "Test acc : [0.945  0.931  0.941  0.9795 0.897 ]\n",
      "8-th round\n",
      "Weight : [1.      0.34846 0.43593 0.41063 0.40412]\n",
      "Loss   : [0.00128 0.02997 0.00202 0.00046 0.08046]\n",
      "Test acc : [0.9565 0.9375 0.9375 0.9805 0.8885]\n",
      "9-th round\n",
      "Weight : [1.      0.26842 0.3634  0.33908 0.32734]\n",
      "Loss   : [0.00288 0.00816 0.0014  0.00194 0.02729]\n",
      "Test acc : [0.975  0.9225 0.9015 0.972  0.879 ]\n",
      "10-th round\n",
      "Weight : [1.      0.18225 0.28375 0.2585  0.2473 ]\n",
      "Loss   : [0.00011 0.00334 0.0003  0.00296 0.00103]\n",
      "Test acc : [0.9855 0.9095 0.8815 0.9455 0.8125]\n",
      "11-th round\n",
      "Weight : [1.      0.08845 0.20859 0.17968 0.16607]\n",
      "Loss   : [0.00828 0.00456 0.00087 0.02606 0.00487]\n",
      "Test acc : [0.9905 0.824  0.7425 0.884  0.747 ]\n",
      "12-th round\n",
      "Weight : [1. 0. 0. 0. 0.]\n",
      "Loss   : [0.00061 0.019   0.00024 0.00281 0.04407]\n",
      "Test acc : [0.9925 0.076  0.029  0.08   0.045 ]\n",
      "13-th round\n",
      "Weight : [1. 0. 0. 0. 0.]\n",
      "Loss   : [0.00052 0.0033  0.00012 0.00535 0.00212]\n",
      "Test acc : [0.9875 0.1355 0.0415 0.098  0.0795]\n",
      "14-th round\n",
      "Weight : [1. 0. 0. 0. 0.]\n",
      "Loss   : [5.40315e-04 4.67864e-02 3.28598e-05 2.10820e-03 2.00286e-02]\n",
      "Test acc : [0.9935 0.078  0.0165 0.0365 0.028 ]\n",
      "15-th round\n",
      "Weight : [1. 0. 0. 0. 0.]\n",
      "Loss   : [2.18247e-05 7.48122e-02 2.18314e-03 9.38862e-04 3.56238e-04]\n",
      "Test acc : [0.994  0.028  0.0115 0.011  0.018 ]\n",
      "16-th round\n",
      "Weight : [1. 0. 0. 0. 0.]\n",
      "Loss   : [0.00116 0.0103  0.00042 0.00887 0.00243]\n",
      "Test acc : [0.9945 0.0195 0.009  0.005  0.009 ]\n",
      "17-th round\n",
      "Weight : [1. 0. 0. 0. 0.]\n",
      "Loss   : [0.00114 0.00779 0.00252 0.10058 0.00235]\n",
      "Test acc : [0.9945 0.0165 0.01   0.005  0.0105]\n",
      "18-th round\n",
      "Weight : [1. 0. 0. 0. 0.]\n",
      "Loss   : [0.00015 0.02098 0.00026 0.08268 0.00079]\n",
      "Test acc : [0.9945 0.0115 0.008  0.004  0.009 ]\n",
      "19-th round\n",
      "Weight : [1. 0. 0. 0. 0.]\n",
      "Loss   : [0.00013 0.00291 0.00077 0.00194 0.00032]\n",
      "Test acc : [0.9945 0.0105 0.007  0.004  0.008 ]\n"
     ]
    }
   ],
   "source": [
    "#=== Non-IID case \n",
    "#=== parameters for Schemes\n",
    "selected_agent_index = 0\n",
    "num_rounds = 20\n",
    "epochs = 1\n",
    "\n",
    "#=== parameters for training and testing\n",
    "num_clients = 5\n",
    "batch_size = 32\n",
    "homogeneity = False\n",
    "\n",
    "#train_loader, test_loader = get_specific_non_IID_loader(num_clients,batch_size,homogeneity) \n",
    "train_loader, test_loader = get_non_IID_loader_digit_trios(num_clients,batch_size,homogeneity)\n",
    "#cumulator not done yet\n",
    "cumulator = base.Cumulator()\n",
    "cumulator.on()\n",
    "\n",
    "run_weight_erosion_non_IID(train_loader,test_loader,num_clients,batch_size,selected_agent_index,num_rounds,epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uHwoRWubPL8M",
    "outputId": "8df12da3-5489-4806-e013-9ee030549f3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight alpha for agent 0 : 1.0\n",
      "Weight alpha for agent 1 : 0.9728857278823853\n",
      "Weight alpha for agent 2 : 0.9722303152084351\n",
      "Weight alpha for agent 3 : 0.9726162552833557\n",
      "Weight alpha for agent 4 : 0.9697747826576233\n",
      "0-th round\n",
      "average train loss 0.18 | test loss 0.143 | test acc: 0.958\n",
      "Weight alpha for agent 0 : 1.0\n",
      "Weight alpha for agent 1 : 0.9249209761619568\n",
      "Weight alpha for agent 2 : 0.9247506260871887\n",
      "Weight alpha for agent 3 : 0.9261295199394226\n",
      "Weight alpha for agent 4 : 0.9224545359611511\n",
      "1-th round\n",
      "average train loss 0.181 | test loss 0.0819 | test acc: 0.973\n",
      "Weight alpha for agent 0 : 1.0\n",
      "Weight alpha for agent 1 : 0.8674433827400208\n",
      "Weight alpha for agent 2 : 0.8652442693710327\n",
      "Weight alpha for agent 3 : 0.8685817718505859\n",
      "Weight alpha for agent 4 : 0.8642969727516174\n",
      "2-th round\n",
      "average train loss 0.148 | test loss 0.0639 | test acc: 0.979\n",
      "Weight alpha for agent 0 : 1.0\n",
      "Weight alpha for agent 1 : 0.8044438362121582\n",
      "Weight alpha for agent 2 : 0.8012921810150146\n",
      "Weight alpha for agent 3 : 0.807581901550293\n",
      "Weight alpha for agent 4 : 0.801755964756012\n",
      "3-th round\n",
      "average train loss 0.106 | test loss 0.0546 | test acc: 0.982\n",
      "Weight alpha for agent 0 : 1.0\n",
      "Weight alpha for agent 1 : 0.73935866355896\n",
      "Weight alpha for agent 2 : 0.7357823252677917\n",
      "Weight alpha for agent 3 : 0.7435076832771301\n",
      "Weight alpha for agent 4 : 0.7367282509803772\n",
      "4-th round\n",
      "average train loss 0.0912 | test loss 0.0449 | test acc: 0.987\n",
      "Weight alpha for agent 0 : 1.0\n",
      "Weight alpha for agent 1 : 0.6704335808753967\n",
      "Weight alpha for agent 2 : 0.6672360897064209\n",
      "Weight alpha for agent 3 : 0.6761763095855713\n",
      "Weight alpha for agent 4 : 0.6687278747558594\n",
      "5-th round\n",
      "average train loss 0.073 | test loss 0.0435 | test acc: 0.986\n",
      "Weight alpha for agent 0 : 1.0\n",
      "Weight alpha for agent 1 : 0.6000097990036011\n",
      "Weight alpha for agent 2 : 0.600730299949646\n",
      "Weight alpha for agent 3 : 0.6104015707969666\n",
      "Weight alpha for agent 4 : 0.6017027497291565\n",
      "6-th round\n",
      "average train loss 0.0585 | test loss 0.0408 | test acc: 0.987\n",
      "Weight alpha for agent 0 : 1.0\n",
      "Weight alpha for agent 1 : 0.529778003692627\n",
      "Weight alpha for agent 2 : 0.5315803289413452\n",
      "Weight alpha for agent 3 : 0.54287189245224\n",
      "Weight alpha for agent 4 : 0.5328662395477295\n",
      "7-th round\n",
      "average train loss 0.0333 | test loss 0.0396 | test acc: 0.987\n",
      "Weight alpha for agent 0 : 1.0\n",
      "Weight alpha for agent 1 : 0.457562118768692\n",
      "Weight alpha for agent 2 : 0.4576094150543213\n",
      "Weight alpha for agent 3 : 0.4704960584640503\n",
      "Weight alpha for agent 4 : 0.4579174518585205\n",
      "8-th round\n",
      "average train loss 0.0897 | test loss 0.0381 | test acc: 0.988\n",
      "Weight alpha for agent 0 : 1.0\n",
      "Weight alpha for agent 1 : 0.38227468729019165\n",
      "Weight alpha for agent 2 : 0.38512519001960754\n",
      "Weight alpha for agent 3 : 0.39739882946014404\n",
      "Weight alpha for agent 4 : 0.38675808906555176\n",
      "9-th round\n",
      "average train loss 0.0117 | test loss 0.0383 | test acc: 0.987\n"
     ]
    }
   ],
   "source": [
    "# IID case: all the clients have images of all the classes\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "num_clients = 5\n",
    "num_rounds = 10\n",
    "epochs = 1\n",
    "batch_size = 32\n",
    "distance_penalty = 0.05\n",
    "size_penalty = 2\n",
    "selected_agent_index = 0\n",
    "\n",
    "# weight_vector\n",
    "\n",
    "weight_vector = np.ones(num_clients)\n",
    "\n",
    "# Creating decentralized datasets\n",
    "\n",
    "traindata = datasets.MNIST('./data', train=True, download=True,\n",
    "                       transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "                       )\n",
    "traindata_split = torch.utils.data.random_split(traindata, [int(traindata.data.shape[0] / num_clients) for _ in range(num_clients)])\n",
    "train_loader = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in traindata_split]\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data', train=False, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        ), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Instantiate models and optimizers\n",
    "shared_model = Net().cuda()\n",
    "client_models = [Net().cuda() for _ in range(num_clients)]\n",
    "for model in client_models:\n",
    "    model.load_state_dict(shared_model.state_dict())\n",
    "\n",
    "opt = [optim.SGD(model.parameters(), lr=0.1) for model in client_models]\n",
    "\n",
    "grad_vector = [None for _ in range(num_clients)]\n",
    "# Runnining Weight Erosion \n",
    "\n",
    "for r in range(num_rounds):\n",
    "    # client update\n",
    "    loss = 0\n",
    "    for i in range(num_clients):\n",
    "        loss_tmp, grad_vector[i] = client_update(client_models[i], opt[i], train_loader[i], epoch=epochs)\n",
    "        loss += loss_tmp\n",
    "        d_rel = grad_vector[0].euclidian_distance(grad_vector[i])\n",
    "        weight_vector[i] = compute_weight(weight_vector[i], r + 1, d_rel, len(train_loader[i]), batch_size, distance_penalty, size_penalty)\n",
    "        print(f\"Weight alpha for agent {i} : {weight_vector[i]}\")\n",
    "    \n",
    "    # Weight Erosion Scheme \n",
    "    weighted_mean_gradient = weighted_average_gradients(grad_vector, weight_vector)\n",
    "    shared_model = update_grad(shared_model, weighted_mean_gradient, 0.1)\n",
    "    \n",
    "    # Share model to all agents \n",
    "    share_weight_erosion_model(shared_model, client_models)\n",
    "    \n",
    "    # Evalutate on the global test set (for now)\n",
    "    test_loss, acc = evaluate(shared_model, test_loader)\n",
    "\n",
    "    print('%d-th round' % r)\n",
    "    print('average train loss %0.3g | test loss %0.3g | test acc: %0.3f' % (loss / num_clients, test_loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NL8mMJcePL8S",
    "outputId": "5a6061cc-b91b-4fb4-f223-58bac1e5aa42"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'relative_distance_vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d1fb91a16d80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mloss_tmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_tmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0md_rel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelative_distance_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselected_agent_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mweight_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_rel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistance_penalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_penalty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Weight alpha for agent {i} : {weight_vector[i]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'relative_distance_vector' is not defined"
     ]
    }
   ],
   "source": [
    "# NON-IID case: every client has images of two categories chosen from [0, 1], [2, 3], [4, 5], [6, 7], or [8, 9].\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "num_clients = 5\n",
    "num_rounds = 10\n",
    "epochs = 1\n",
    "batch_size = 32\n",
    "distance_penalty = 0.05\n",
    "size_penalty = 2\n",
    "selected_agent_index = 0\n",
    "\n",
    "# weight_vector\n",
    "\n",
    "weight_vector = np.ones(num_clients)\n",
    "\n",
    "# Creating decentralized datasets\n",
    "\n",
    "traindata = datasets.MNIST('./data', train=True, download=True,\n",
    "                       transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "                       )\n",
    "target_labels = torch.stack([traindata.targets == i for i in range(10)])\n",
    "target_labels_split = []\n",
    "for i in range(5):\n",
    "    target_labels_split += torch.split(torch.where(target_labels[(2 * i):(2 * (i + 1))].sum(0))[0], int(60000 / num_clients))\n",
    "traindata_split = [torch.utils.data.Subset(traindata, tl) for tl in target_labels_split]\n",
    "train_loader = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in traindata_split]\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data', train=False, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        ), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Instantiate models and optimizers\n",
    "\n",
    "to_share_model = Net().cuda()\n",
    "client_models = [Net().cuda() for _ in range(num_clients)]\n",
    "for model in client_models:\n",
    "    model.load_state_dict(to_share_model.state_dict())\n",
    "\n",
    "opt = [optim.SGD(model.parameters(), lr=0.1) for model in client_models]\n",
    "\n",
    "\n",
    "for r in range(num_rounds):\n",
    "    # client update\n",
    "    loss = 0\n",
    "    for i in range(num_clients):\n",
    "        loss_tmp, grad_vector[i] = client_update(client_models[i], opt[i], train_loader[i], epoch=epochs)\n",
    "        loss += loss_tmp\n",
    "        d_rel = relative_distance_vector(grad_vector[selected_agent_index], grad_vector[i])\n",
    "        weight_vector[i] = compute_weight(weight_vector[i], r + 1, d_rel, len(train_loader[i]), batch_size, distance_penalty, size_penalty)\n",
    "        print(f\"Weight alpha for agent {i} : {weight_vector[i]}\")\n",
    "    \n",
    "    # Weight Erosion Scheme \n",
    "    weighted_mean_gradient = weighted_average_gradients(grad_vector, weight_vector)\n",
    "    to_share_model = update_grad(to_share_model, weighted_mean_gradient, 0.1)\n",
    "    \n",
    "    # Share model to all agents \n",
    "    share_weight_erosion_model(to_share_model, client_models)\n",
    "    \n",
    "    # Evalutate on the global test set (for now)\n",
    "    test_loss, acc = evaluate(to_share_model, test_loader)\n",
    "\n",
    "    print('%d-th round' % r)\n",
    "    print('average train loss %0.3g | test loss %0.3g | test acc: %0.3f' % (loss / num_clients, test_loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tR6rSf5SPL8X"
   },
   "outputs": [],
   "source": [
    "# IID case: all the clients have images of all the classes\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "num_clients = 5\n",
    "num_rounds = 5\n",
    "epochs = 1\n",
    "batch_size = 32\n",
    "\n",
    "# Communication matrix\n",
    "\n",
    "comm_matrix = np.ones((num_clients, num_clients)) / num_clients\n",
    "# comm_matrix = np.eye(num_clients)\n",
    "\n",
    "# Creating decentralized datasets\n",
    "\n",
    "traindata = datasets.MNIST('./data', train=True, download=True,\n",
    "                       transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "                       )\n",
    "traindata_split = torch.utils.data.random_split(traindata, [int(traindata.data.shape[0] / num_clients) for _ in range(num_clients)])\n",
    "train_loader = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in traindata_split]\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data', train=False, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        ), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Instantiate models and optimizers\n",
    "\n",
    "global_model = Net().cuda()\n",
    "client_models = [Net().cuda() for _ in range(num_clients)]\n",
    "for model in client_models:\n",
    "    model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "opt = [optim.SGD(model.parameters(), lr=0.1) for model in client_models]\n",
    "\n",
    "# Runnining Decentralized training\n",
    "\n",
    "for r in range(num_rounds):\n",
    "    # client update\n",
    "    loss = 0\n",
    "    for i in range(num_clients):\n",
    "        loss += client_update(client_models[i], opt[i], train_loader[i], epoch=epochs)\n",
    "    \n",
    "    # diffuse params\n",
    "    diffuse_params(client_models, comm_matrix)\n",
    "\n",
    "    average_models(global_model, client_models)\n",
    "    test_loss, acc = evaluate(global_model, test_loader)\n",
    "    \n",
    "    print('%d-th round' % r)\n",
    "    print('average train loss %0.3g | test loss %0.3g | test acc: %0.3f' % (loss / num_clients, test_loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aAdtJAnEy_Kw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "launch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
