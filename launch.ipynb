{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "launch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sESj8k1APL8E"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
        "        self.fc1 = nn.Linear(2048, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "\n",
        "def client_update(client_model, optimizer, train_loader, epoch=5):\n",
        "    \"\"\"Train a client_model on the train_loder data.\"\"\"\n",
        "    client_model.train()\n",
        "    for e in range(epoch):\n",
        "        gradients = []\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "            optimizer.zero_grad()\n",
        "            output = client_model(data)\n",
        "            loss = F.nll_loss(output, target)\n",
        "            loss.backward()\n",
        "            #gradients.append(get_gradient_dict_from_model(client_model))\n",
        "      \n",
        "    return loss.item(), get_gradient_dict_from_model(client_model)\n",
        "\n",
        "def relative_distance(grad_selected_agent, grad_current_agent):\n",
        "    \"\"\"Computes the relative distance between the current model and the global model\"\"\"\n",
        "    return torch.dist(grad_selected_agent, grad_current_agent, 2) / torch.norm(grad_selected_agent, 2)\n",
        "\n",
        "def compute_weight(alpha_prev, round, relative_distance, data_size, batch_size, distance_penalty, size_penalty):\n",
        "    \"\"\"Computes the weight alpha for round r\"\"\"\n",
        "    size_factor = (1 + distance_penalty * math.floor(((round - 1) * batch_size) / data_size)) \n",
        "    distance_factor = distance_penalty * relative_distance\n",
        "    alpha = alpha_prev - size_factor * distance_factor \n",
        "    return max(0,alpha)\n",
        "\n",
        "\n",
        "def get_gradient_dict_from_model(model):\n",
        "    \"\"\" Returns a list of the gradient for each parameter\"\"\"\n",
        "    dict_gradients= {}\n",
        "    for name, param in model.named_parameters():\n",
        "        dict_gradients[name] = param.grad.data.cpu()\n",
        "    return dict_gradients\n",
        "\n",
        "\n",
        "def relative_distance_vector(grad_selected_agent, grad_current_agent):\n",
        "    \"\"\"Computes the relative euclidean distance of the flattened tensor between the current model and the global model\"\"\"\n",
        "    grad_selected = get_gradient_tensor(list(grad_selected_agent.values()))\n",
        "    grad_current = get_gradient_tensor(list(grad_current_agent.values()))\n",
        "    return torch.dist(grad_selected, grad_current, 2) / torch.norm(grad_selected, 2)\n",
        "\n",
        "\n",
        "# Compute the new weighted average gradient\n",
        "def weighted_average_gradients(agents_gradients, weights):\n",
        "    \"\"\"Compute the weighted average gradient.\"\"\"\n",
        "    weighted_averages = {}\n",
        "    for key in agents_gradients[0].keys():\n",
        "      weighted_averages[key] = weighted_average_from_key(key, agents_gradients, weights)\n",
        "    return weighted_averages\n",
        "\n",
        "def weighted_average_from_key(key, agents_gradients, weights):\n",
        "  n = 0\n",
        "  d = 0 \n",
        "  for idx, g_dict in enumerate(agents_gradients) :\n",
        "    n += g_dict[key] * weights[idx]\n",
        "    d += weights[idx]\n",
        "  return n / d\n",
        "\n",
        "\n",
        "def compute_weight(alpha_prev, round, relative_distance, data_size, batch_size, distance_penalty, size_penalty):\n",
        "    \"\"\"Computes the weight alpha for round r\"\"\"\n",
        "    size_factor = (1 + size_penalty * math.floor(((round - 1) * batch_size) / data_size)) \n",
        "    distance_factor = distance_penalty * relative_distance\n",
        "    alpha = alpha_prev - size_factor * distance_factor \n",
        "    return max(0,alpha)\n",
        "\n",
        "# Utilitary functions to get NN model\n",
        "def get_gradient_list_from_model(model):\n",
        "    \"\"\" Returns a list of the gradient for each parameter\"\"\"\n",
        "    parameters = list(model.parameters())\n",
        "    gradients = list(map(lambda p : p.grad.data.cpu(), parameters))\n",
        "    return gradients\n",
        "\n",
        "def get_gradient_tensor(gradient_list):\n",
        "    \"\"\" Returns an aggregated tensor of all the gradients for one model\"\"\"\n",
        "    gradients = list(map(lambda g : torch.flatten(g), gradient_list))\n",
        "    return torch.cat(gradients, 0)\n",
        "\n",
        "def update_grad(model, optim, gradient, alpha): \n",
        "  for name, param in model.named_parameters():\n",
        "    param.data = gradient[name].cuda() * alpha\n",
        "  #optim.step()\n",
        "  return model \n",
        "\n",
        "\n",
        "def share_weight_erosion_model(global_model, client_models):\n",
        "  for model in client_models:\n",
        "        model.load_state_dict(global_model.state_dict())\n",
        "\n",
        "\n",
        "\n",
        "def average_models(global_model, client_models):\n",
        "    \"\"\"Average models across all clients.\"\"\"\n",
        "    global_dict = global_model.state_dict()\n",
        "    for k in global_dict.keys():\n",
        "        print(k)\n",
        "        global_dict[k] = torch.stack([client_models[i].state_dict()[k] for i in range(len(client_models))], 0).mean(0)\n",
        "    global_model.load_state_dict(global_dict)\n",
        "\n",
        "\n",
        "def evaluate(global_model, data_loader):\n",
        "    \"\"\"Compute loss and accuracy of a model on a data_loader.\"\"\"\n",
        "    global_model.eval()\n",
        "    loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "            output = global_model(data)\n",
        "            loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    loss /= len(data_loader.dataset)\n",
        "    acc = correct / len(data_loader.dataset)\n",
        "\n",
        "    return loss, acc"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ubHxgLQqEt7",
        "outputId": "fd45bc2c-14d9-4efc-bc71-43197133f247"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "uHwoRWubPL8M",
        "outputId": "78b31dfb-be3e-4793-e8c8-eced5a4f7fe2"
      },
      "source": [
        "# IID case: all the clients have images of all the classes\n",
        "\n",
        "# Hyperparameters\n",
        "\n",
        "num_clients = 5\n",
        "num_rounds = 50\n",
        "epochs = 1\n",
        "batch_size = 32\n",
        "distance_penalty = 0.05\n",
        "size_penalty = 2\n",
        "selected_agent_index = 0\n",
        "\n",
        "# weight_vector\n",
        "\n",
        "weight_vector = np.ones(num_clients)\n",
        "\n",
        "# Creating decentralized datasets\n",
        "\n",
        "traindata = datasets.MNIST('./data', train=True, download=True,\n",
        "                       transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
        "                       )\n",
        "testdata = datasets.MNIST('./data', train=False, download=True,\n",
        "                       transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
        "                       )\n",
        "traindata_split = torch.utils.data.random_split(traindata, [int(traindata.data.shape[0] / num_clients) for _ in range(num_clients)])\n",
        "train_loader = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in traindata_split]\n",
        "\n",
        "# Create corresponding test dataset\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('./data', train=False, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
        "        ), batch_size=batch_size, shuffle=True)\n",
        "#testdata_split = torch.utils.data.random_split(testdata, [int(testdata.data.shape[0] / num_clients) for _ in range(num_clients)])\n",
        "#test_loader = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in testdata_split]\n",
        "\n",
        "# Instantiate models and optimizers\n",
        "\n",
        "global_model = Net().cuda()\n",
        "client_models = [Net().cuda() for _ in range(num_clients)]\n",
        "for model in client_models:\n",
        "    model.load_state_dict(global_model.state_dict())\n",
        "\n",
        "opt = [optim.SGD(model.parameters(), lr=0.1) for model in client_models]\n",
        "\n",
        "grad_vector = [None for _ in range(num_clients)]\n",
        "# Runnining Weight Erosion \n",
        "\n",
        "for r in range(num_rounds):\n",
        "    # client update\n",
        "    loss = 0\n",
        "    for i in range(num_clients):\n",
        "        loss_tmp, grad_vector[i] = client_update(client_models[i], opt[i], train_loader[i], epoch=epochs)\n",
        "        loss += loss_tmp\n",
        "        d_rel = relative_distance_vector(grad_vector[selected_agent_index], grad_vector[i])\n",
        "        weight_vector[i] = compute_weight(weight_vector[i], r + 1, d_rel, len(train_loader[i]), batch_size, distance_penalty, size_penalty)\n",
        "        print(\"alpha : \", weight_vector[i])\n",
        "\n",
        "    # Compute the Weight Erosion Model\n",
        "    weighted_average_gradient = weighted_average_gradients(grad_vector, weight_vector)\n",
        "    global_model = update_grad(client_models[selected_agent_index], opt[selected_agent_index], weighted_average_gradient, 0.1)\n",
        "    \n",
        "    # Share computed model to all agents\n",
        "    share_weight_erosion_model(global_model, client_models)\n",
        "    \n",
        "    test_loss, acc = evaluate(global_model, test_loader)\n",
        "\n",
        "    print('%d-th round' % r)\n",
        "    print('average train loss %0.3g | test loss %0.3g | test acc: %0.3f' % (loss / num_clients, test_loss, acc))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "alpha :  1.0\n",
            "alpha :  0.9551077485084534\n",
            "alpha :  0.9354026913642883\n",
            "alpha :  0.9320671558380127\n",
            "alpha :  0.944237232208252\n",
            "0-th round\n",
            "average train loss 2.31 | test loss 2.3 | test acc: 0.098\n",
            "alpha :  1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-31de5d867c77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_clients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mloss_tmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_tmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0md_rel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelative_distance_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselected_agent_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-95c9a3a78873>\u001b[0m in \u001b[0;36mclient_update\u001b[0;34m(client_model, optimizer, train_loader, epoch)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0;31m#gradients.append(get_gradient_dict_from_model(client_model))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "NL8mMJcePL8S",
        "outputId": "432e2ee8-fc78-48bd-d27d-c5215a964635"
      },
      "source": [
        "# NON-IID case: every client has images of two categories chosen from [0, 1], [2, 3], [4, 5], [6, 7], or [8, 9].\n",
        "\n",
        "# Hyperparameters\n",
        "\n",
        "num_clients = 5\n",
        "num_rounds = 5\n",
        "epochs = 1\n",
        "batch_size = 32\n",
        "distance_penalty = 0.05\n",
        "size_penalty = 2\n",
        "#selected_agent_index = 0\n",
        "\n",
        "# Creating decentralized datasets\n",
        "\n",
        "traindata = datasets.MNIST('./data', train=True, download=True,\n",
        "                       transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
        "                       )\n",
        "target_labels = torch.stack([traindata.targets == i for i in range(10)])\n",
        "target_labels_split = []\n",
        "for i in range(5):\n",
        "    target_labels_split += torch.split(torch.where(target_labels[(2 * i):(2 * (i + 1))].sum(0))[0], int(60000 / num_clients))\n",
        "traindata_split = [torch.utils.data.Subset(traindata, tl) for tl in target_labels_split]\n",
        "train_loader = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in traindata_split]\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('./data', train=False, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
        "        ), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Instantiate models and optimizers\n",
        "\n",
        "for selected_agent_index in range(5):\n",
        "  # weight_vector\n",
        "\n",
        "  weight_vector = np.ones(num_clients)\n",
        "  print(f\"Computing accuracy for selected agent {selected_agent_index}\")\n",
        "  global_model = Net().cuda()\n",
        "  client_models = [Net().cuda() for _ in range(num_clients)]\n",
        "  for model in client_models:\n",
        "      model.load_state_dict(global_model.state_dict())\n",
        "\n",
        "  opt = [optim.SGD(model.parameters(), lr=0.1) for model in client_models]\n",
        "\n",
        "  grad_vector = [None for _ in range(num_clients)]\n",
        "  # Runnining Weight Erosion \n",
        "\n",
        "  for r in range(num_rounds):\n",
        "      # client update\n",
        "      loss = 0\n",
        "      for i in range(num_clients):\n",
        "          loss_tmp, grad_vector[i] = client_update(client_models[i], opt[i], train_loader[i], epoch=epochs)\n",
        "          loss += loss_tmp\n",
        "      for i in range(num_clients):\n",
        "          d_rel = relative_distance_vector(grad_vector[selected_agent_index], grad_vector[i])\n",
        "          weight_vector[i] = compute_weight(weight_vector[i], r + 1, d_rel, len(train_loader[i]), batch_size, distance_penalty, size_penalty)\n",
        "          print(f\"alpha for agent {i}: {weight_vector[i]}\" )\n",
        "\n",
        "      # Compute the Weight Erosion Model\n",
        "      weighted_mean_gradient = weighted_average_gradients(grad_vector, weight_vector)\n",
        "      global_model = update_grad(client_models[selected_agent_index], weighted_mean_gradient, 0.01)\n",
        "      \n",
        "      # Share computed model to all agents\n",
        "      share_weight_erosion_model(global_model, client_models)\n",
        "      \n",
        "      test_loss, acc = evaluate(global_model, test_loader)\n",
        "\n",
        "      print('%d-th round' % r)\n",
        "      print('average train loss %0.3g | test loss %0.3g | test acc: %0.3f' % (loss / num_clients, test_loss, acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Computing accuracy for selected agent 0\n",
            "alpha for agent 0: 1.0\n",
            "alpha for agent 1: 0.9796341061592102\n",
            "alpha for agent 2: 0.9208574295043945\n",
            "alpha for agent 3: 0.9196247458457947\n",
            "alpha for agent 4: 0.9230688214302063\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-55f909a1c3f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0;31m# Compute the Weight Erosion Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m       \u001b[0mweighted_mean_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweighted_average_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m       \u001b[0mglobal_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselected_agent_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweighted_mean_gradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m       \u001b[0;31m# Share computed model to all agents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: update_grad() missing 1 required positional argument: 'alpha'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "tR6rSf5SPL8X",
        "outputId": "4e6690ea-ec26-470b-8972-bc4fee134cab"
      },
      "source": [
        "# IID case: all the clients have images of all the classes\n",
        "\n",
        "# Hyperparameters\n",
        "\n",
        "num_clients = 5\n",
        "num_rounds = 5\n",
        "epochs = 1\n",
        "batch_size = 32\n",
        "\n",
        "# Communication matrix\n",
        "\n",
        "comm_matrix = np.ones((num_clients, num_clients)) / num_clients\n",
        "# comm_matrix = np.eye(num_clients)\n",
        "\n",
        "# Creating decentralized datasets\n",
        "\n",
        "traindata = datasets.MNIST('./data', train=True, download=True,\n",
        "                       transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
        "                       )\n",
        "traindata_split = torch.utils.data.random_split(traindata, [int(traindata.data.shape[0] / num_clients) for _ in range(num_clients)])\n",
        "train_loader = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in traindata_split]\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('./data', train=False, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
        "        ), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Instantiate models and optimizers\n",
        "\n",
        "global_model = Net().cuda()\n",
        "client_models = [Net().cuda() for _ in range(num_clients)]\n",
        "for model in client_models:\n",
        "    model.load_state_dict(global_model.state_dict())\n",
        "\n",
        "opt = [optim.SGD(model.parameters(), lr=0.1) for model in client_models]\n",
        "\n",
        "# Runnining Decentralized training\n",
        "\n",
        "for r in range(num_rounds):\n",
        "    # client update\n",
        "    loss = 0\n",
        "    for i in range(num_clients):\n",
        "        loss += client_update(client_models[i], opt[i], train_loader[i], epoch=epochs)\n",
        "    \n",
        "    # diffuse params\n",
        "    diffuse_params(client_models, comm_matrix)\n",
        "\n",
        "    average_models(global_model, client_models)\n",
        "    test_loss, acc = evaluate(global_model, test_loader)\n",
        "    \n",
        "    print('%d-th round' % r)\n",
        "    print('average train loss %0.3g | test loss %0.3g | test acc: %0.3f' % (loss / num_clients, test_loss, acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-9a4fb6c7b0be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_clients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mclient_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# diffuse params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +=: 'int' and 'tuple'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf4YArdhqEt8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}